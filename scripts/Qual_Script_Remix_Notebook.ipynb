{"cells":[{"cell_type":"markdown","id":"2722e299","metadata":{"id":"2722e299"},"source":["\n","# Qual — Option C (Remix)\n","Minimal notebook that loads **World Data 2.0** and runs a few methods:\n","- **word-frequency** (top unigrams/bigrams)\n","- **simple-sentiment** (lexicon-style polarity)\n","- **semantic-embeddings** (KMeans clusters on sentence embeddings; TF-IDF fallback)\n","\n","> This version **does not auto-build a text column**. It requires an existing text column.\n"]},{"cell_type":"code","execution_count":null,"id":"bd531449","metadata":{"id":"bd531449"},"outputs":[],"source":["\n","# =========================\n","# CONFIG (EDIT HERE)\n","# =========================\n","DATA_URL = \"https://github.com/zachtilton/intermediate_analytics_ai/blob/main/World%20Data%202.0%20-%20Data.csv\"\n","\n","# Choose 1–2 methods in order, e.g.: [\"word-frequency\", \"simple-sentiment\"]\n","METHODS = [\"word-frequency\", \"simple-sentiment\"]\n","\n","# Columns\n","DOC_ID_COL = \"Country\"   # document id\n","TEXT_COL = \"text\"        # REQUIRED existing column name (case-insensitive OK)\n","\n","# Parameters\n","TOP_N = 30   # top-N unigrams/bigrams\n","K = 4        # clusters when using semantic-embeddings\n","\n","print(\"Config set. Edit METHODS/columns above as needed.\")\n"]},{"cell_type":"code","execution_count":null,"id":"8a05ce11","metadata":{"id":"8a05ce11"},"outputs":[],"source":["\n","import pandas as pd\n","import numpy as np\n","import re\n","from collections import Counter\n","\n","def to_raw_github_url(url: str) -> str:\n","    if \"github.com\" in url and \"raw.githubusercontent.com\" not in url:\n","        url = url.replace(\"github.com/\", \"raw.githubusercontent.com/\")\n","        url = url.replace(\"/blob/\", \"/\")\n","    return url\n","\n","STOPWORDS = {\n","    'a','an','the','and','or','but','if','then','else','of','to','in','on','for','with',\n","    'is','are','was','were','be','been','being','at','by','from','as','it','this','that',\n","    'these','those','there','here','we','you','they','he','she','them','his','her','their',\n","    'i','me','my','our','ours','your','yours','us'\n","}\n","\n","TOKEN_RE = re.compile(r\"[a-zA-Z]{2,}\")\n","\n","def tokenize(text):\n","    toks = [t.lower() for t in TOKEN_RE.findall(text or '') if t.lower() not in STOPWORDS]\n","    return toks\n","\n","def make_bigrams(tokens):\n","    return [f\"{tokens[i]} {tokens[i+1]}\" for i in range(len(tokens)-1)]\n","\n","SENTIMENT = {\n","    'good': 2, 'great': 3, 'excellent': 4, 'positive': 2, 'benefit': 2, 'improve': 2,\n","    'bad': -2, 'poor': -2, 'negative': -2, 'harm': -3, 'worse': -2, 'risk': -1\n","}\n","\n","def doc_sentiment(tokens):\n","    return sum(SENTIMENT.get(t, 0) for t in tokens)\n","\n","pd.set_option(\"display.width\", 120)\n","pd.set_option(\"display.max_columns\", 50)\n"]},{"cell_type":"code","execution_count":null,"id":"588dac8a","metadata":{"id":"588dac8a"},"outputs":[],"source":["\n","RAW_URL = to_raw_github_url(DATA_URL)\n","df = pd.read_csv(RAW_URL)\n","df.columns = [c.strip() for c in df.columns]\n","\n","print(\"Loaded shape:\", df.shape)\n","print(\"Columns:\")\n","print(list(df.columns))\n","\n","print(\"\\nPreview:\")\n","display(df.head(3))\n"]},{"cell_type":"code","execution_count":null,"id":"d5e1e7ed","metadata":{"id":"d5e1e7ed"},"outputs":[],"source":["\n","# Use existing text column only (no auto-build)\n","if DOC_ID_COL not in df.columns:\n","    raise ValueError(f\"DOC_ID_COL '{DOC_ID_COL}' not found in CSV columns.\")\n","\n","# case-insensitive resolution of TEXT_COL\n","col_map = {c.lower(): c for c in df.columns}\n","tc = TEXT_COL if TEXT_COL in df.columns else col_map.get(TEXT_COL.lower())\n","\n","if tc is None:\n","    raise ValueError(f\"TEXT_COL '{TEXT_COL}' not found. Please set TEXT_COL to the exact column name containing text.\")\n","\n","docs = df[[DOC_ID_COL, tc]].rename(columns={DOC_ID_COL:\"doc_id\", tc:\"text\"}).copy()\n","docs = docs.dropna(subset=[\"text\"])\n","docs[\"text\"] = docs[\"text\"].astype(str).str.strip()\n","docs = docs[docs[\"text\"] != \"\"]\n","\n","print(\"Docs shape:\", docs.shape)\n","display(docs.head(5))\n"]},{"cell_type":"code","execution_count":null,"id":"f0261d05","metadata":{"id":"f0261d05"},"outputs":[],"source":["\n","methods = [m.strip().lower() for m in METHODS]\n","valid = {\"word-frequency\", \"simple-sentiment\", \"semantic-embeddings\"}\n","if any(m not in valid for m in methods):\n","    raise ValueError(f\"METHODS must be subset of {valid}\")\n","\n","top_unigrams = None\n","top_bigrams = None\n","doc_polarity = None\n","clusters_table = None\n","cluster_summary = None\n","\n","# word-frequency\n","if \"word-frequency\" in methods:\n","    uni = Counter(); bi = Counter()\n","    for _, row in docs.iterrows():\n","        toks = tokenize(row[\"text\"])\n","        uni.update(toks)\n","        bi.update(make_bigrams(toks))\n","    top_unigrams = pd.DataFrame(uni.most_common(TOP_N), columns=[\"term\",\"freq\"])\n","    top_bigrams = pd.DataFrame(bi.most_common(TOP_N), columns=[\"term\",\"freq\"])\n","    print(\"\\n[WORD FREQUENCY] Top unigrams:\"); display(top_unigrams.head(10))\n","    print(\"Top bigrams:\"); display(top_bigrams.head(10))\n","\n","# simple-sentiment\n","if \"simple-sentiment\" in methods:\n","    rows = []\n","    for _, row in docs.iterrows():\n","        toks = tokenize(row[\"text\"])\n","        rows.append({\"doc_id\": row[\"doc_id\"], \"sentiment\": doc_sentiment(toks)})\n","    doc_polarity = pd.DataFrame(rows)\n","    print(\"\\n[SIMPLE SENTIMENT] (lexicon-style, illustrative)\"); display(doc_polarity.head(10))\n","    print(\"Note: Very rough heuristic; interpret directionally.\")\n","\n","# semantic-embeddings\n","if \"semantic-embeddings\" in methods:\n","    texts = docs[\"text\"].tolist()\n","    try:\n","        from sentence_transformers import SentenceTransformer\n","        from sklearn.cluster import KMeans\n","        model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","        emb = model.encode(texts, normalize_embeddings=True, show_progress_bar=False)\n","        km = KMeans(n_clusters=K, n_init=10, random_state=42).fit(emb)\n","        labels = km.labels_\n","        print(\"\\n[SEMANTIC EMBEDDINGS] Used 'all-MiniLM-L6-v2'.\")\n","    except Exception as e:\n","        print(\"\\n[SEMANTIC EMBEDDINGS] sentence-transformers unavailable; TF-IDF + KMeans fallback.\")\n","        from sklearn.feature_extraction.text import TfidfVectorizer\n","        from sklearn.cluster import KMeans\n","        vec = TfidfVectorizer(max_features=5000, stop_words=\"english\", ngram_range=(1,2))\n","        X = vec.fit_transform(texts)\n","        km = KMeans(n_clusters=K, n_init=10, random_state=42).fit(X)\n","        labels = km.labels_\n","\n","    clusters_table = docs[[\"doc_id\",\"text\"]].copy()\n","    clusters_table[\"cluster\"] = labels\n","    clusters_table[\"snippet\"] = clusters_table[\"text\"].str.slice(0, 140).str.replace(\"\\n\",\" \", regex=False)\n","\n","    summary_rows = []\n","    for c in sorted(clusters_table[\"cluster\"].unique()):\n","        group = clusters_table[clusters_table[\"cluster\"] == c]\n","        ex = group[\"snippet\"].head(3).tolist()\n","        summary_rows.append({\"cluster\": int(c), \"size\": int(len(group)), \"examples\": ex})\n","    cluster_summary = pd.DataFrame(summary_rows).sort_values(\"cluster\")\n","\n","    print(\"\\n[CLUSTERS] doc_id → cluster (preview):\"); display(clusters_table.head(10))\n","    print(\"\\n[CLUSTER SUMMARY]\"); display(cluster_summary)\n"]},{"cell_type":"code","execution_count":null,"id":"5427619f","metadata":{"id":"5427619f"},"outputs":[],"source":["\n","print(\"\\n===== Compact Results Ready =====\")\n","if top_unigrams is not None: print(f\"top_unigrams shape: {top_unigrams.shape}\")\n","if top_bigrams is not None: print(f\"top_bigrams shape: {top_bigrams.shape}\")\n","if doc_polarity is not None: print(f\"doc_polarity shape: {doc_polarity.shape}\")\n","if clusters_table is not None: print(f\"clusters_table shape: {clusters_table.shape}\")\n","if cluster_summary is not None: print(f\"cluster_summary shape: {cluster_summary.shape}\")\n","\n","if \"word-frequency\" in methods and top_unigrams is not None:\n","    common_terms = \", \".join(top_unigrams[\"term\"].head(5).tolist())\n","    print(f\"\\n[INTERPRETATION] Frequent terms include: {common_terms}. Consider what shared themes these reflect.\")\n","\n","if \"simple-sentiment\" in methods and doc_polarity is not None:\n","    avg = doc_polarity[\"sentiment\"].mean()\n","    direction = \"positive\" if avg > 0 else \"negative\" if avg < 0 else \"neutral\"\n","    print(f\"[INTERPRETATION] Average sentiment skews {direction} (mean ≈ {avg:.2f}); treat as directional only.\")\n","\n","if \"semantic-embeddings\" in methods and cluster_summary is not None:\n","    print(\"[INTERPRETATION] Clusters suggest semantically similar groups. Review example snippets to label each cluster.\")\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1KNzkzsdH_T_lQLiLN6WLbzOg_reASKp6","timestamp":1758125529051}]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}